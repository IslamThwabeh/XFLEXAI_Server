# services/openai_service.py
import time
import base64
import requests
import os
from PIL import Image
from io import BytesIO
from config import Config

OPENAI_AVAILABLE = False
client = None
openai_error_message = ""
openai_last_check = 0

def init_openai():
    """
    Initialize OpenAI client and test model availability.
    Sets OPENAI_AVAILABLE, client, openai_error_message, openai_last_check.
    """
    global OPENAI_AVAILABLE, client, openai_error_message, openai_last_check
    
    print("ğŸš¨ OPENAI INIT: Starting OpenAI initialization...")
    
    try:
        from openai import OpenAI
        print("ğŸš¨ OPENAI INIT: OpenAI package imported successfully")
        
        # Get API key from Config
        api_key = Config.OPENAI_API_KEY
        print(f"ğŸš¨ OPENAI INIT: Config.OPENAI_API_KEY = {api_key[:20]}..." if api_key else "ğŸš¨ OPENAI INIT: Config.OPENAI_API_KEY = None")
        print(f"ğŸš¨ OPENAI INIT: API Key exists: {bool(api_key)}")
        print(f"ğŸš¨ OPENAI INIT: API Key length: {len(api_key) if api_key else 0}")

        if not api_key or api_key == "your-api-key-here":
            openai_error_message = "OpenAI API key not configured"
            print(f"ğŸš¨ OPENAI INIT: âŒ API key check failed - not configured or still default")
            OPENAI_AVAILABLE = False
            return False

        print("ğŸš¨ OPENAI INIT: Creating OpenAI client...")
        client = OpenAI(api_key=api_key)
        print("ğŸš¨ OPENAI INIT: OpenAI client created successfully")

        try:
            print("ğŸš¨ OPENAI INIT: Testing model availability...")
            models = client.models.list()
            model_ids = [m.id for m in models.data]
            print(f"ğŸš¨ OPENAI INIT: Found {len(model_ids)} models")
            print(f"ğŸš¨ OPENAI INIT: First few models: {model_ids[:5]}")
            
            if "gpt-4o" not in model_ids:
                openai_error_message = "GPT-4o model not available in your account"
                print(f"ğŸš¨ OPENAI INIT: âŒ GPT-4o not found in available models")
                OPENAI_AVAILABLE = False
                return False

            print("ğŸš¨ OPENAI INIT: âœ… GPT-4o model found!")
            OPENAI_AVAILABLE = True
            openai_error_message = ""
            openai_last_check = time.time()
            print("ğŸš¨ OPENAI INIT: âœ… OpenAI initialized successfully!")
            return True
            
        except Exception as e:
            error_msg = str(e)
            print(f"ğŸš¨ OPENAI INIT: âŒ Model list error: {error_msg}")
            if "insufficient_quota" in error_msg:
                openai_error_message = "Account has no API credits. Please add funds to your OpenAI API account."
            elif "invalid_api_key" in error_msg:
                openai_error_message = "Invalid API key. Please check your OPENAI_API_KEY environment variable."
            elif "rate limit" in error_msg.lower():
                openai_error_message = "Rate limit exceeded. Please try again later."
            else:
                openai_error_message = f"OpenAI API test failed: {error_msg}"
            OPENAI_AVAILABLE = False
            return False

    except ImportError as e:
        print(f"ğŸš¨ OPENAI INIT: âŒ OpenAI package import error: {e}")
        openai_error_message = f"OpenAI package not installed: {e}"
        OPENAI_AVAILABLE = False
        return False
    except Exception as e:
        print(f"ğŸš¨ OPENAI INIT: âŒ General initialization error: {str(e)}")
        openai_error_message = f"OpenAI initialization error: {str(e)}"
        OPENAI_AVAILABLE = False
        return False

def analyze_with_openai(image_str, image_format, timeframe=None, previous_analysis=None, user_analysis=None, action_type="chart_analysis"):
    """
    Analyze an image or text using OpenAI and enforce a character limit in responses.
    Mirrors the original analyze logic.
    """
    global client

    if not OPENAI_AVAILABLE:
        raise RuntimeError(f"OpenAI not available: {openai_error_message}")

    if action_type == "user_analysis_feedback":
        char_limit = 800
        analysis_prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ­Ù„ÙŠÙ„ ÙÙ†ÙŠ. Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØªÙ‚Ø¯ÙŠÙ… Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¨Ù†Ø§Ø¡Ø©:

ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{user_analysis}

**Ø§Ù„ØªØ²Ù… Ø§Ù„ØµØ§Ø±Ù… Ø¨Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„ØªØ§Ù„ÙŠØ©:**
1. Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² {char_limit} Ø­Ø±Ù ØªØ­Øª Ø£ÙŠ Ø¸Ø±Ù
2. Ù‚Ø¯Ù… Ù†Ù‚Ø§Ø· Ù‚ÙˆØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„
3. Ù‚Ø¯Ù… Ù†Ù‚Ø§Ø· ØªØ­Ø³ÙŠÙ† Ù…Ø¹ Ø´Ø±Ø­ Ù…ÙˆØ¬Ø²
4. Ù‚Ø¯Ù… Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©

**ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯ Ø§Ù„Ø£Ø­Ø±Ù ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ø­Ø¯ {char_limit} Ø­Ø±Ù.**
"""
        max_tokens = char_limit // 2 + 50

    elif timeframe == "H4" and previous_analysis:
        char_limit = 800
        analysis_prompt = f"""
Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙÙ†ÙŠ Ù…Ø­ØªØ±Ù. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹ Ù…ÙˆØ¬Ø²Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø·Ø§Ø±ÙŠÙ†.

Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¨Ù‚ (15 Ø¯Ù‚ÙŠÙ‚Ø©): {previous_analysis[:150]}...

**Ø§Ù„ØªØ²Ù… Ø§Ù„ØµØ§Ø±Ù… Ø¨Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„ØªØ§Ù„ÙŠØ©:**
1. Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² {char_limit} Ø­Ø±Ù ØªØ­Øª Ø£ÙŠ Ø¸Ø±Ù
2. Ø¯Ù…Ø¬ Ø§Ù„Ø±Ø¤ÙŠØ§Øª Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±ÙŠÙ†
3. ØªÙ‚Ø¯ÙŠÙ… ØªÙˆØµÙŠØ© ØªØ¯Ø§ÙˆÙ„ ÙˆØ§Ø­Ø¯Ø© ÙˆØ§Ø¶Ø­Ø©
4. Ø°ÙƒØ± Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© Ø¨Ø§Ø®ØªØµØ§Ø±

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙÙŠ 3 Ù†Ù‚Ø§Ø· ÙÙ‚Ø·:**
1. Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ÙƒÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±ÙŠÙ†
2. Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
3. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©

**ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯ Ø§Ù„Ø£Ø­Ø±Ù ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ø­Ø¯ {char_limit} Ø­Ø±Ù.**
"""
        max_tokens = char_limit // 2 + 50

    else:
        char_limit = 600
        analysis_prompt = f"""
Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙÙ†ÙŠ Ù…Ø­ØªØ±Ù. Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…Ø®ØªØµØ±Ø§Ù‹ Ù„Ù„ØºØ§ÙŠØ© Ù„Ù„Ø´Ø§Ø±Øª.

**Ø§Ù„ØªØ²Ù… Ø§Ù„ØµØ§Ø±Ù… Ø¨Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„ØªØ§Ù„ÙŠØ©:**
1. Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² {char_limit} Ø­Ø±Ù ØªØ­Øª Ø£ÙŠ Ø¸Ø±Ù
2. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙÙ‚Ø·
3. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ù…Ø®ØªØµØ±Ø© Ø¬Ø¯Ø§Ù‹

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙÙŠ 4 Ù†Ù‚Ø§Ø· ÙÙ‚Ø·:**
1. Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹Ø§Ù… (Ø³Ø·Ø± ÙˆØ§Ø­Ø¯)
2. Ø£Ù‡Ù… Ù…Ø³ØªÙˆÙ‰ Ø¯Ø¹Ù… ÙˆÙ…Ù‚Ø§ÙˆÙ…Ø© (Ø³Ø·Ø± ÙˆØ§Ø­Ø¯)
3. ØªÙˆØµÙŠØ© ØªØ¯Ø§ÙˆÙ„ ÙˆØ§Ø¶Ø­Ø© (Ø³Ø·Ø± ÙˆØ§Ø­Ø¯)
4. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© (Ø³Ø·Ø± ÙˆØ§Ø­Ø¯)

**ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯ Ø§Ù„Ø£Ø­Ø±Ù ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ø­Ø¯ {char_limit} Ø­Ø±Ù.**
"""
        max_tokens = char_limit // 2 + 50

    if not client:
        raise RuntimeError("OpenAI client not initialized")

    try:
        if image_str:
            print(f"ğŸš¨ OPENAI ANALYSIS: Analyzing image with {action_type}")
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": f"Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙÙ†ÙŠ Ù…Ø­ØªØ±Ù. Ø§Ù„ØªØ²Ù… Ø§Ù„ØµØ§Ø±Ù… Ø¨Ø¹Ø¯Ù… ØªØ¬Ø§ÙˆØ² {char_limit} Ø­Ø±Ù ÙÙŠ Ø±Ø¯Ùƒ."},
                    {"role": "user", "content": [
                        {"type": "text", "text": analysis_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/{image_format.lower()};base64,{image_str}", "detail": "high"}}
                    ]}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
        else:
            print(f"ğŸš¨ OPENAI ANALYSIS: Analyzing text with {action_type}")
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": f"Ø£Ù†Øª Ù…Ø­Ù„Ù„ ÙÙ†ÙŠ Ù…Ø­ØªØ±Ù. Ø§Ù„ØªØ²Ù… Ø§Ù„ØµØ§Ø±Ù… Ø¨Ø¹Ø¯Ù… ØªØ¬Ø§ÙˆØ² {char_limit} Ø­Ø±Ù ÙÙŠ Ø±Ø¯Ùƒ."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )

        analysis = response.choices[0].message.content.strip()
        print(f"ğŸš¨ OPENAI ANALYSIS: âœ… Analysis completed, length: {len(analysis)} chars")

        # backup enforcement of character limit
        if len(analysis) > char_limit + 100:
            print(f"ğŸš¨ OPENAI ANALYSIS: Analysis too long ({len(analysis)}), retrying with shorter version")
            retry_prompt = f"""
Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ ({len(analysis)} Ø­Ø±Ù). Ø£Ø¹Ø¯ ÙƒØªØ§Ø¨ØªÙ‡ Ù…Ø¹ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø¹Ø¯Ù… ØªØ¬Ø§ÙˆØ² {char_limit} Ø­Ø±Ù:

{analysis}
"""
            retry_response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": f"Ø§Ø®ØªØµØ§Ø± Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ {char_limit} Ø­Ø±Ù."},
                    {"role": "user", "content": retry_prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            analysis = retry_response.choices[0].message.content.strip()
            print(f"ğŸš¨ OPENAI ANALYSIS: âœ… Retry completed, new length: {len(analysis)} chars")

        return analysis

    except Exception as e:
        print(f"ğŸš¨ OPENAI ANALYSIS: âŒ Analysis failed: {str(e)}")
        raise RuntimeError(f"OpenAI analysis failed: {str(e)}")

def load_image_from_url(image_url):
    """Load and encode image from URL and return (b64string, format) or (None, None)"""
    try:
        print(f"ğŸš¨ IMAGE LOAD: Loading image from {image_url}")
        response = requests.get(image_url, timeout=10)
        if response.status_code == 200:
            img = Image.open(BytesIO(response.content))
            if img.format in ['PNG', 'JPEG', 'JPG']:
                buffered = BytesIO()
                img_format = img.format if img.format else 'JPEG'
                img.save(buffered, format=img_format)
                b64_data = base64.b64encode(buffered.getvalue()).decode("utf-8")
                print(f"ğŸš¨ IMAGE LOAD: âœ… Image loaded successfully, format: {img_format}, size: {len(b64_data)} chars")
                return b64_data, img_format
        print(f"ğŸš¨ IMAGE LOAD: âŒ Failed to load image, status: {response.status_code}")
        return None, None
    except Exception as e:
        print(f"ğŸš¨ IMAGE LOAD: âŒ Error loading image: {e}")
        return None, None
